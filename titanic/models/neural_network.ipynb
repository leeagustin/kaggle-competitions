{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (Score: 0.70574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class for Titanic CSV dataset\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        self.X = data.drop('Survived', axis=1)\n",
    "        self.y = data['Survived']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            self.X.loc[idx].values.astype(np.float32),\n",
    "            self.y[idx].astype(np.float32)\n",
    "        ]\n",
    "    \n",
    "    def get_splits(self, n_train=0.8):\n",
    "        train_size = int(0.8 * len(train_data))\n",
    "        valid_size = len(train_data) - train_size\n",
    "        return random_split(train_data, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and validation datasets\n",
    "train_data = TitanicDataset('../data/preprocessed_train.csv')\n",
    "train_data, valid_data = train_data.get_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data loaders\n",
    "train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_data, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, activation=nn.ReLU):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.layer = nn.Linear(n_inputs, n_outputs)\n",
    "        self.activation = activation()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            DenseLayer(n_inputs, 128),\n",
    "            DenseLayer(128, 64),\n",
    "            DenseLayer(64, 32),\n",
    "            DenseLayer(32, 16),\n",
    "            DenseLayer(16, 1, activation=nn.Sigmoid)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        return X\n",
    "    \n",
    "    def validate(self, dl):\n",
    "        all_preds, all_targets = [], []\n",
    "        for i, (inputs, targets) in enumerate(dl):\n",
    "            preds = model(inputs).reshape(-1, 1).detach().numpy().round()\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "        all_preds, all_targets = np.vstack(all_preds), np.vstack(all_targets)\n",
    "        acc = accuracy_score(all_targets, all_preds)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer): Sequential(\n",
       "    (0): DenseLayer(\n",
       "      (layer): Linear(in_features=10, out_features=128, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): DenseLayer(\n",
       "      (layer): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (2): DenseLayer(\n",
       "      (layer): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): DenseLayer(\n",
       "      (layer): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): DenseLayer(\n",
       "      (layer): Linear(in_features=16, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create neural network model\n",
    "model = MLP(10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/100 Epochs: \n",
      "Loss: 0.42981446\n",
      "Validation Accuracy Score: 50.27932960893855%\n",
      "\n",
      "20/100 Epochs: \n",
      "Loss: 0.6129082\n",
      "Validation Accuracy Score: 65.36312849162012%\n",
      "\n",
      "30/100 Epochs: \n",
      "Loss: 0.43251526\n",
      "Validation Accuracy Score: 67.59776536312849%\n",
      "\n",
      "40/100 Epochs: \n",
      "Loss: 0.6966103\n",
      "Validation Accuracy Score: 68.15642458100558%\n",
      "\n",
      "50/100 Epochs: \n",
      "Loss: 0.39492115\n",
      "Validation Accuracy Score: 68.71508379888269%\n",
      "\n",
      "60/100 Epochs: \n",
      "Loss: 0.73012197\n",
      "Validation Accuracy Score: 69.27374301675978%\n",
      "\n",
      "70/100 Epochs: \n",
      "Loss: 0.32848075\n",
      "Validation Accuracy Score: 70.39106145251397%\n",
      "\n",
      "80/100 Epochs: \n",
      "Loss: 0.8577157\n",
      "Validation Accuracy Score: 72.06703910614524%\n",
      "\n",
      "90/100 Epochs: \n",
      "Loss: 0.46775386\n",
      "Validation Accuracy Score: 72.62569832402235%\n",
      "\n",
      "100/100 Epochs: \n",
      "Loss: 0.3566735\n",
      "Validation Accuracy Score: 75.41899441340783%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "# loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# train/validate model\n",
    "for epoch in range(1, epochs+1):\n",
    "    # train model\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs).squeeze()\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # validate model\n",
    "    if epoch%10 == 0:\n",
    "        print(str(epoch) + \"/\" + str(epochs)  + \" Epochs: \")\n",
    "        print(\"Loss: \" + str(loss.detach().numpy()))\n",
    "        acc = model.validate(valid_dl)\n",
    "        print(\"Validation Accuracy Score: \" + str(acc*100) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_df = pd.read_csv('../data/preprocessed_test.csv')\n",
    "test_data = torch.tensor(test_df.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['PassengerId'] = test_df['PassengerId']\n",
    "submission['Survived'] = model(test_data).round().int().numpy()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission.to_csv('../submissions/neural_network.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd06be0ac93fc83aa16398a1f83ff52a1059a5a114c41716045c7d9e7e8b15f1ea5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "6be0ac93fc83aa16398a1f83ff52a1059a5a114c41716045c7d9e7e8b15f1ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
