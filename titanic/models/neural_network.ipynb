{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network (Score: 0.70574)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "MODEL_PARAMS_PATH = 'params/'\n",
    "SUBMISSIONS_PATH = '../submissions/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class for Titanic CSV dataset\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = pd.read_csv(path)\n",
    "        self.X = data.drop('Survived', axis=1)\n",
    "        self.y = data['Survived']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            self.X.loc[idx].values.astype(np.float32),\n",
    "            self.y[idx].astype(np.float32)\n",
    "        ]\n",
    "    \n",
    "    def get_splits(self, n_train=0.8):\n",
    "        train_size = int(0.8 * len(train_data))\n",
    "        valid_size = len(train_data) - train_size\n",
    "        return random_split(train_data, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training and validation datasets\n",
    "train_data = TitanicDataset(os.path.join(DATA_PATH + 'preprocessed_train.csv'))\n",
    "train_data, valid_data = train_data.get_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation data loaders\n",
    "train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_data, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, activation=nn.ReLU):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.layer = nn.Linear(n_inputs, n_outputs)\n",
    "        self.activation = activation()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            DenseLayer(n_inputs, 512),\n",
    "            DenseLayer(512, 256),\n",
    "            DenseLayer(256, 128),\n",
    "            DenseLayer(128, 64),\n",
    "            DenseLayer(64, 32),\n",
    "            DenseLayer(32, 16),\n",
    "            DenseLayer(16, 8),\n",
    "            DenseLayer(8, 1, activation=nn.Sigmoid)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        return X\n",
    "    \n",
    "    def validate(self, dl):\n",
    "        all_preds, all_targets = [], []\n",
    "        for i, (inputs, targets) in enumerate(dl):\n",
    "            preds = model(inputs).reshape(-1, 1).detach().numpy().round()\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(targets)\n",
    "        all_preds, all_targets = np.vstack(all_preds), np.vstack(all_targets)\n",
    "        acc = accuracy_score(all_targets, all_preds)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer): Sequential(\n",
       "    (0): DenseLayer(\n",
       "      (layer): Linear(in_features=10, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1): DenseLayer(\n",
       "      (layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (2): DenseLayer(\n",
       "      (layer): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): DenseLayer(\n",
       "      (layer): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): DenseLayer(\n",
       "      (layer): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (5): DenseLayer(\n",
       "      (layer): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (6): DenseLayer(\n",
       "      (layer): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (7): DenseLayer(\n",
       "      (layer): Linear(in_features=8, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view model structure\n",
    "model = MLP(10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 1e+00\n",
      "Achieved at Epoch: 10/100\n",
      "Lowest Loss: 37.5\n",
      "Highest Training Accuracy: 61.38%\n",
      "Highest Validation Accuracy: 62.57%\n",
      "\n",
      "Learning Rate: 1e-01\n",
      "Achieved at Epoch: 10/100\n",
      "Lowest Loss: 37.5\n",
      "Highest Training Accuracy: 61.38%\n",
      "Highest Validation Accuracy: 62.57%\n",
      "\n",
      "Learning Rate: 1e-02\n",
      "Achieved at Epoch: 10/100\n",
      "Lowest Loss: 0.49677604\n",
      "Highest Training Accuracy: 61.38%\n",
      "Highest Validation Accuracy: 62.57%\n",
      "\n",
      "Learning Rate: 1e-03\n",
      "Achieved at Epoch: 90/100\n",
      "Lowest Loss: 0.50955665\n",
      "Highest Training Accuracy: 75.84%\n",
      "Highest Validation Accuracy: 79.33%\n",
      "\n",
      "Learning Rate: 1e-04\n",
      "Achieved at Epoch: 40/100\n",
      "Lowest Loss: 0.31632942\n",
      "Highest Training Accuracy: 67.98%\n",
      "Highest Validation Accuracy: 68.72%\n",
      "\n",
      "Learning Rate: 1e-05\n",
      "Achieved at Epoch: 10/100\n",
      "Lowest Loss: 0.70137584\n",
      "Highest Training Accuracy: 61.38%\n",
      "Highest Validation Accuracy: 62.57%\n",
      "\n",
      "Learning Rate: 1e-06\n",
      "Achieved at Epoch: 100/100\n",
      "Lowest Loss: 0.6587238\n",
      "Highest Training Accuracy: 55.20%\n",
      "Highest Validation Accuracy: 55.31%\n",
      "\n",
      "----------- BEST MODEL -----------\n",
      "Learning Rate: 1e-03\n",
      "Achieved at Epoch: 90/100\n",
      "Lowest Loss: 0.50955665\n",
      "Highest Training Accuracy: 75.84%\n",
      "Highest Validation Accuracy: 79.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lrs = [10**-i for i in range(7)]\n",
    "epochs = 100\n",
    "\n",
    "global_lowest_loss = float('inf')\n",
    "global_highest_valid_acc = 0\n",
    "\n",
    "# train/validate model\n",
    "for lr in lrs:\n",
    "    # reset model\n",
    "    model = MLP(10)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_epochs = 0\n",
    "    local_lowest_loss = float('inf')\n",
    "    local_highest_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # train model\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs).squeeze()\n",
    "            loss = criterion(preds, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validate model\n",
    "        if epoch%10 == 0:\n",
    "            train_acc = model.validate(train_dl)\n",
    "            valid_acc = model.validate(valid_dl)\n",
    "\n",
    "            if loss < local_lowest_loss and valid_acc > local_highest_valid_acc:\n",
    "                local_best_epochs = epoch\n",
    "                local_lowest_loss = loss\n",
    "                local_highest_train_acc = train_acc\n",
    "                local_highest_valid_acc = valid_acc\n",
    "                \n",
    "            if loss < global_lowest_loss and valid_acc > global_highest_valid_acc:\n",
    "                global_best_lr = lr\n",
    "                global_best_epochs = epoch\n",
    "                global_lowest_loss = loss\n",
    "                global_highest_train_acc = train_acc\n",
    "                global_highest_valid_acc = valid_acc\n",
    "                torch.save(model.state_dict(), os.path.join(MODEL_PARAMS_PATH, 'neural_network.pt'))\n",
    "                \n",
    "    print(\"Learning Rate: %.0e\" % (lr))\n",
    "    print(\"Achieved at Epoch: \" + str(local_best_epochs) + \"/\" + str(epochs))\n",
    "    print(\"Lowest Loss: \" + str(local_lowest_loss.detach().numpy()))\n",
    "    print(\"Highest Training Accuracy: %.2f%%\" % (local_highest_train_acc*100))\n",
    "    print(\"Highest Validation Accuracy: %.2f%%\" % (local_highest_valid_acc*100))\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"----------- BEST MODEL -----------\")\n",
    "print(\"Learning Rate: %.0e\" % (global_best_lr))\n",
    "print(\"Achieved at Epoch: \" + str(global_best_epochs) + \"/\" + str(epochs))\n",
    "print(\"Lowest Loss: \" + str(global_lowest_loss.detach().numpy()))\n",
    "print(\"Highest Training Accuracy: %.2f%%\" % (global_highest_train_acc*100))\n",
    "print(\"Highest Validation Accuracy: %.2f%%\" % (global_highest_valid_acc*100))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'preprocessed_test.csv'))\n",
    "test_data = torch.tensor(test_df.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model and \n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_PARAMS_PATH, 'neural_network.pt')))\n",
    "model.eval()\n",
    "\n",
    "# predict using test dataset\n",
    "submission = pd.DataFrame()\n",
    "submission['PassengerId'] = test_df['PassengerId']\n",
    "submission['Survived'] = model(test_data).round().int().numpy()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "submission.to_csv(os.path.join(SUBMISSIONS_PATH, 'neural_network.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd06be0ac93fc83aa16398a1f83ff52a1059a5a114c41716045c7d9e7e8b15f1ea5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "6be0ac93fc83aa16398a1f83ff52a1059a5a114c41716045c7d9e7e8b15f1ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
